import pandas as pd
import requests
import re
from io import BytesIO
from sentence_transformers import SentenceTransformer, util
import pymorphy2
import functools

# ---------- –º–æ–¥–µ–ª—å –∏ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–±–æ—Ä ----------

@functools.lru_cache(maxsize=1)
def get_model():
    import os
    import zipfile
    import gdown

    model_path = "fine_tuned_model"
    model_zip  = "fine_tuned_model.zip"
    file_id    = "1RR15OMLj9vfSrVa1HN-dRU-4LbkdbRRf"  # –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∑–∞–º–µ–Ω–∏—Ç–µ

    if not os.path.exists(model_path):
        gdown.download(f"https://drive.google.com/uc?id={file_id}", model_zip, quiet=False)
        with zipfile.ZipFile(model_zip, 'r') as zf:
            zf.extractall(model_path)

    return SentenceTransformer(model_path)

@functools.lru_cache(maxsize=1)
def get_morph():
    return pymorphy2.MorphAnalyzer()

# ---------- —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ----------

def preprocess(text):
    return re.sub(r"\s+", " ", str(text).lower().strip())

def lemmatize(word):
    return get_morph().parse(word)[0].normal_form

@functools.lru_cache(maxsize=10000)
def lemmatize_cached(word):
    return lemmatize(word)

SYNONYM_GROUPS = []

SYNONYM_DICT = {}
if SYNONYM_GROUPS:  # –∑–∞—â–∏—Ç–∞ –æ—Ç –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞
    for group in SYNONYM_GROUPS:
        lemmas = {lemmatize(w.lower()) for w in group}
        for lemma in lemmas:
            SYNONYM_DICT[lemma] = lemmas

GITHUB_CSV_URLS = [
    "https://raw.githubusercontent.com/d3lskx05/data-assistanttestx/main/data4.xlsx",
    "https://raw.githubusercontent.com/skatzrsk/semantic-assistant/main/data21.xlsx",
    "https://raw.githubusercontent.com/skatzrsk/semantic-assistant/main/data31.xlsx",
]

def split_by_slash(phrase: str):
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –≤–∏–¥–∞:
      '–ø–æ—Ç–µ—Ä—è–ª —Å–∏–º–∫—É/–Ω–æ–º–µ—Ä|—Å–∏–º –∫–∞—Ä—Ç—É' -> ['–ø–æ—Ç–µ—Ä—è–ª —Å–∏–º–∫—É', '–ø–æ—Ç–µ—Ä—è–ª –Ω–æ–º–µ—Ä', '—Å–∏–º –∫–∞—Ä—Ç—É']
    """
    phrase = phrase.strip()
    parts  = []
    for segment in phrase.split("|"):
        segment = segment.strip()
        if "/" in segment:
            tokens = [p.strip() for p in segment.split("/") if p.strip()]
            if len(tokens) == 2:
                # –ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—â–∏–π –ø—Ä–µ—Ñ–∏–∫—Å/—Å—É—Ñ—Ñ–∏–∫—Å
                m = re.match(r"^(.*?\b)?(\w+)\s*/\s*(\w+)(\b.*?)?$", segment)
                if m:
                    prefix = (m.group(1) or "").strip()
                    first  = m.group(2).strip()
                    second = m.group(3).strip()
                    suffix = (m.group(4) or "").strip()
                    parts.append(" ".join(filter(None, [prefix, first,  suffix])))
                    parts.append(" ".join(filter(None, [prefix, second, suffix])))
                    continue
            # fallback: –ø—Ä–æ—Å—Ç–æ –±–µ—Ä—ë–º –≤—Å—ë, —á—Ç–æ —Å–ª–µ–≤–∞/—Å–ø—Ä–∞–≤–∞ –æ—Ç /
            parts.extend(tokens)
        else:
            parts.append(segment)
    return [p for p in parts if p]

# ---------- –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ----------

def load_excel(url):
    resp = requests.get(url)
    if resp.status_code != 200:
        raise ValueError(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}")

    df = pd.read_excel(BytesIO(resp.content))

    # –ü—Ä–æ–≤–µ—Ä–∫–∏ –Ω–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    if "phrase" not in df.columns:
         raise KeyError("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ 'phrase' –≤ —Ñ–∞–π–ª–µ.")
    topic_cols = [c for c in df.columns if c.lower().startswith("topics")]
    if not topic_cols:
        raise KeyError("–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–æ–ª–æ–Ω–∫–∏ topics*")

    # –°–ø–∏—Å–æ–∫ —Ç–µ–º
    df["topics"] = df[topic_cols].agg(lambda x: [v for v in x if pd.notna(v) and str(v).strip()], axis=1)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª
    df["phrase"] = df["phrase"].astype(str)  # –∑–∞—â–∏—Ç–∞ –æ—Ç NaN
    df["raw_phrase"] = df["phrase"]          # üëà –æ—Ä–∏–≥–∏–Ω–∞–ª –¥–æ —Ä–∞–∑–±–∏–µ–Ω–∏—è

    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø–æ–¥—Ñ—Ä–∞–∑—ã
    df["phrase_list"] = df["phrase"].apply(split_by_slash)
    df = df.explode("phrase_list", ignore_index=True)

    # –ü–æ–¥—Ñ—Ä–∞–∑–∞, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –∏—â–µ–º
    df["phrase_sub"] = df["phrase_list"].fillna("").astype(str).str.strip()

    # –û—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º –ø—É—Å—Ç—ã–µ
    df = df[df["phrase_sub"] != ""].reset_index(drop=True)

    # –ü–æ–ª–µ–∑–Ω—ã–µ –ø–æ–ª—è
    df["phrase"] = df["phrase_sub"]      # —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –æ—Å—Ç–∞–ª—å–Ω—ã–º –∫–æ–¥–æ–º
    df["phrase_full"] = df["raw_phrase"] # üëà –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤ –≤—ã–¥–∞—á–µ –æ—Ä–∏–≥–∏–Ω–∞–ª (—Å / –∏ |)

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    df["phrase_proc"] = df["phrase"].apply(preprocess)
    df["phrase_lemmas"] = df["phrase_proc"].apply(
        lambda t: {lemmatize_cached(w) for w in re.findall(r"\w+", t)}
    )

    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –ø–æ –ø–æ–¥—Ñ—Ä–∞–∑–∞–º
    model = get_model()
    df.attrs["phrase_embs"] = model.encode(df["phrase_proc"].tolist(), convert_to_tensor=True)

    # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
    if "comment" not in df.columns:
        df["comment"] = ""

    return df[["phrase", "phrase_proc", "phrase_full", "phrase_lemmas", "topics", "comment"]]

def load_all_excels():
    dfs = []
    for url in GITHUB_CSV_URLS:
        try:
            dfs.append(load_excel(url))
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å {url}: {e}")
    if not dfs:
        raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞")
    return pd.concat(dfs, ignore_index=True)

# ---------- —É–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–µ–π ----------

def _score_of(item):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å–ª–æ–≤–æ–π score –∏–∑ –∫–æ—Ä—Ç–µ–∂–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""
    return item[0] if len(item) == 4 else 1.0

def _phrase_full_of(item):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç phrase_full –∏–∑ –∫–æ—Ä—Ç–µ–∂–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞."""
    return item[1] if len(item) == 4 else item[0]

def _topics_of(item):
    return item[2] if len(item) == 4 else item[1]

def _comment_of(item):
    return item[3] if len(item) == 4 else item[2]

def deduplicate_results(results, merge_topics=True, merge_comments=True):
    """
    –£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ phrase_full. –ï—Å–ª–∏ merge_topics=True,
    –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–ø–∏—Å–∫–∏ —Ç–µ–º –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –æ–¥–Ω–æ–≥–æ raw_phrase.
    """
    best = {}
    for item in results:
        key   = _phrase_full_of(item)
        score = _score_of(item)
        if key not in best:
            best[key] = item
        else:
            # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π –ø–æ score
            if score > _score_of(best[key]):
                best[key] = item
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ–º—ã/–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –ø—Ä–∏ –∂–µ–ª–∞–Ω–∏–∏
            if merge_topics:
                merged_topics = list({*map(str, _topics_of(best[key])), *map(str, _topics_of(item))})
                if len(best[key]) == 4:
                    best[key] = ( _score_of(best[key]), key, merged_topics, _comment_of(best[key]) )
                else:
                    best[key] = ( key, merged_topics, _comment_of(best[key]) )
            if merge_comments and _comment_of(item) and _comment_of(item) not in _comment_of(best[key]):
                if len(best[key]) == 4:
                    best[key] = ( _score_of(best[key]), key, _topics_of(best[key]),
                                  (_comment_of(best[key]) + " | " + _comment_of(item)).strip(" |") )
                else:
                    best[key] = ( key, _topics_of(best[key]),
                                  (_comment_of(best[key]) + " | " + _comment_of(item)).strip(" |") )
    return list(best.values())

# ---------- –ø–æ–∏—Å–∫ ----------

def semantic_search(query, df, top_k=5, threshold=0.5):
    model       = get_model()
    query_proc  = preprocess(query)
    query_emb   = model.encode(query_proc, convert_to_tensor=True)
    phrase_embs = df.attrs["phrase_embs"]

    sims = util.pytorch_cos_sim(query_emb, phrase_embs)[0]

    # –°–æ–±–∏—Ä–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    cand = []
    for idx, score in enumerate(sims):
        score_f = float(score)
        if score_f >= threshold:
            row = df.iloc[idx]
            cand.append((score_f, row["phrase_full"], row["topics"], row["comment"]))

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º/–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
    cand = sorted(cand, key=lambda x: x[0], reverse=True)[:top_k * 4]  # –±–µ—Ä—ë–º —á—É—Ç—å –±–æ–ª—å—à–µ –ø–µ—Ä–µ–¥ –¥–µ–¥—É–ø–æ–º

    # –î–µ–¥—É–ø –ø–æ –∏—Å—Ö–æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–µ
    return deduplicate_results(cand)

def keyword_search(query, df):
    query_proc   = preprocess(query)
    query_words  = re.findall(r"\w+", query_proc)
    query_lemmas = [lemmatize_cached(w) for w in query_words]

    matched = []
    for row in df.itertuples():
        # row.phrase_lemmas: set
        lemma_match = all(
            any(ql in SYNONYM_DICT.get(pl, {pl}) for pl in row.phrase_lemmas)
            for ql in query_lemmas
        )
        partial_match = all(q in row.phrase_proc for q in query_words)
        if lemma_match or partial_match:
            matched.append((row.phrase_full, row.topics, row.comment))

    return deduplicate_results(matched)

# ---------- —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è ----------

def filter_by_topics(results, selected_topics):
    if not selected_topics:
        return results
    sel = set(selected_topics)

    filtered = []
    for item in results:
        if isinstance(item, tuple) and len(item) == 4:
            score, phrase, topics, comment = item
            topics_list = topics if isinstance(topics, (list, tuple, set)) else [topics]
            if sel & set(topics_list):
                filtered.append((score, phrase, topics, comment))
        elif isinstance(item, tuple) and len(item) == 3:
            phrase, topics, comment = item
            topics_list = topics if isinstance(topics, (list, tuple, set)) else [topics]
            if sel & set(topics_list):
                filtered.append((phrase, topics, comment))
    return filtered
